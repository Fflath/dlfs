# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_Trainer.ipynb.

# %% auto 0
__all__ = ['Optimizer', 'identity', 'Callback', 'MetricsCB', 'ProgressCB', 'CancelFitException', 'CancelBatchException',
           'CancelEpochException', 'with_callbacks', 'Core', 'MomentumLearner', 'Learner']

# %% ../nbs/02_Trainer.ipynb 2
from .Dataset import *

# %% ../nbs/02_Trainer.ipynb 3
import math,torch,matplotlib.pyplot as plt
from torch import tensor,nn,no_grad
import torch
import torch.nn.functional as F
import fastcore.all as fc
from torch import optim
from copy import copy
from fastprogress import progress_bar,master_bar

import torchvision.transforms.functional as TF


# %% ../nbs/02_Trainer.ipynb 4
class Optimizer():
    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr

    def step(self):
        with no_grad():
            for p in self.params: p -= p.grad * self.lr

    def zero_grad(self):
        for p in self.params: p.grad.data.zero_()

# %% ../nbs/02_Trainer.ipynb 12
def identity(*args):
    if not args: return
    x,*args=args
    return (x,)+tuple(args) if args else x

# %% ../nbs/02_Trainer.ipynb 13
class Callback: order = 0

# %% ../nbs/02_Trainer.ipynb 14
from torcheval.metrics import MulticlassAccuracy,Mean

class MetricsCB(Callback):
    def __init__(self, *ms, **metrics): 
        for o in ms: metrics[type(o).__name__]=o
        self.metrics=metrics
        self.all_metrics=copy(metrics)
        self.all_metrics['loss']=self.loss = Mean()
        
    def _log(self,d): print(d)
        
    def before_fit(self): self.learner.metrics=self
    def before_epoch(self): [o.reset() for o in self.all_metrics.values()]
    def after_epoch(self): 
        log={k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
        log['epoch'] = self.learner.epoch
        log['train']=self.learner.model.training
        self._log(log)
    def after_batch(self):
        x,y=self.learner.xb, self.learner.yb
        for m in self.metrics.values(): m.update(self.learner.preds, y)
        self.loss.update(self.learner.loss, weight=len(x))

# %% ../nbs/02_Trainer.ipynb 15
class ProgressCB(Callback):
    order=MetricsCB.order+1
    def __init__(self, plot=False): self.plot=plot
    def before_fit(self):
        self.learner.epochs = self.mbar = master_bar(self.learner.epochs)
        self.first = True
        if hasattr(self.learner,'metrics'): self.learner.metrics._log=self._log
        self.losses=[]
    def _log(self,d): 
        if self.first:
            self.mbar.write(list(d), table=True)
            self.first = False
        self.mbar.write(list(d.values()), table=True)
    def before_epoch(self): self.learner.dl = progress_bar(self.learner.dl, leave=False, parent = self.mbar)
    def after_batch(self):
        self.learner.dl.comment = f'{self.learner.loss: .3f}'
        if self.plot and hasattr(self.learner, 'metrics') and self.learner.model.training:
            self.losses.append(self.learner.loss.item())
            self.mbar.update_graph([[fc.L.range(self.losses), self.losses]])
        

# %% ../nbs/02_Trainer.ipynb 16
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass
class CancelEpochException(Exception): pass

# %% ../nbs/02_Trainer.ipynb 17
class with_callbacks:
    def __init__(self,nm): self.nm=nm
    
    def __call__(self,f):
        def _f(o, *args, **kwargs):
            try:
                o.callback(f'before_{self.nm}')
                f(o, *args, **kwargs)
                o.callback(f'after_{self.nm}')
            except globals()[f'Cancel{self.nm.title()}Exception']: pass
        return _f

# %% ../nbs/02_Trainer.ipynb 18
class Core:
    def predict(self): self.learner.preds = self.learner.model(self.learner.xb)
    def get_loss(self): self.learner.loss = self.learner.loss_func(self.learner.preds, self.learner.yb)
    def backward(self): self.learner.loss.backward()
    def step(self): self.learner.opt_func.step()
    def zero_grad(self): self.learner.opt.zero_grad()

# %% ../nbs/02_Trainer.ipynb 19
class MomentumLearner(Core):
    def __init__(self, mom=0.85):
        self.mom = mom
        super().__init__()

    def zero_grad(self):
        with torch.no_grad():
            for p in self.learner.model.parameters(): p.grad *= self.mom

# %% ../nbs/02_Trainer.ipynb 20
class Learner:
    def __init__(self, model, core, dls, callbacks, loss_func, lr, opt_func=optim.SGD): 
        fc.store_attr()
        for cb in self.callbacks: cb.learner=self
        self.callbacks.sort
        self.core.learner=self

    @with_callbacks('batch')
    def one_batch(self):
        self.xb,self.yb = self.batch
        self.core.predict()
        self.core.get_loss()
        if self.model.training:
            self.core.backward()
            self.core.step()
            self.core.zero_grad()

    def one_epoch(self, train):
        self.model.training = train
        self.dl = self.dls.train if train else self.dls.valid
        self._one_epoch()
    
    @with_callbacks('epoch')
    def _one_epoch(self):
        for self.num,self.batch in enumerate(self.dl): self.one_batch()

        
    def fit(self, n_epochs):
        self.n_epochs = n_epochs
        self.epochs = range(n_epochs)
        self.opt = self.opt_func(self.model.parameters(), self.lr)
        self._fit()
            
    @with_callbacks('fit')
    def _fit(self):
        for self.epoch in self.epochs:
            self.one_epoch(True)
            with torch.no_grad(): self.one_epoch(False)

    def callback(self, method_nm): 
        for cb in self.callbacks: getattr(cb, method_nm, identity)()   
